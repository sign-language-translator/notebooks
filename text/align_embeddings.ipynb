{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_vectors(path: str):\n",
    "    tokens, vectors = [], []\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        n_lines, n_dim = map(int, f.readline().split())\n",
    "        for line in tqdm(f, total=n_lines, desc='Loading vectors'):\n",
    "            word, *vector = line.split()\n",
    "            if not len(vector) == n_dim:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "            vectors.append(np.array(vector, dtype=np.float32))\n",
    "\n",
    "    print(\"loaded: \", len(tokens), len(vectors))\n",
    "    return tokens, np.stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d58da53fb04f5b87ab9d27802194a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading vectors:   0%|          | 0/200123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded:  200117 200117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c800d0390f4b28aff85e4512afd9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading vectors:   0%|          | 0/200604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded:  200604 200604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((9416, 300), (9416, 300))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"ur_to_en.json\", \"r\") as f:\n",
    "    ur_to_en = json.load(f)\n",
    "    src_toks, tgt_toks = list(ur_to_en.keys()), [tok.lower() for tok in ur_to_en.values()]\n",
    "\n",
    "src_tokens, src_vectors = load_vectors(\"temp/align/cc.ur.300.vec\")\n",
    "src_idxs = np.array([src_tokens.index(tok) for tok in src_toks])\n",
    "src_vecs = src_vectors[src_idxs]\n",
    "\n",
    "tgt_tokens, tgt_vectors = load_vectors(\"temp/align/cc.en.300.vec\")\n",
    "tgt_tokens = [t.lower() for t in tgt_tokens]\n",
    "tgt_idxs = np.array([tgt_tokens.index(tok) for tok in tgt_toks])\n",
    "tgt_vecs = tgt_vectors[tgt_idxs]\n",
    "\n",
    "src_vecs.shape, tgt_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.arange(len(src_vecs))\n",
    "np.random.shuffle(idxs)\n",
    "n_train = int(0.8 * len(src_vecs))\n",
    "train_idxs = idxs[:n_train]\n",
    "valid_idxs = idxs[n_train:]\n",
    "\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = learn_transformation(src_vecs[train_idxs], tgt_vecs[train_idxs])\n",
    "# R = learn_transformation(src_vecs, tgt_vecs)\n",
    "# torch.save(torch.tensor(R), \"temp/align/ur_to_en_300x300.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  لائی target:  brought\n",
      "original similarity:  0.09436258\n",
      "aligned similarity:  0.4547023\n"
     ]
    }
   ],
   "source": [
    "src_word = np.random.choice(np.array(src_toks)[valid_idxs], 1)[0]\n",
    "src_vec = src_vecs[src_toks.index(src_word)]\n",
    "\n",
    "tgt_word = ur_to_en[src_word].lower()\n",
    "tgt_vec = tgt_vecs[tgt_toks.index(tgt_word)]\n",
    "\n",
    "print(\"source: \", src_word, \"target: \", tgt_word)\n",
    "print(\"original similarity: \", (src_vec @ tgt_vec) / (np.linalg.norm(src_vec) * np.linalg.norm(tgt_vec)))\n",
    "\n",
    "print(\"aligned similarity: \", ((src_vec @ R) @ tgt_vec) / (np.linalg.norm(src_vec) * np.linalg.norm(tgt_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1884, 200604)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sims = normalized(src_vecs @ R) @ normalized(tgt_vectors).T\n",
    "val_sims = all_sims[valid_idxs]\n",
    "val_sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ٹین',\n",
       " [('pancake', 0.32896447),\n",
       "  ('k-', 0.31401414),\n",
       "  ('loft', 0.30195698),\n",
       "  ('boys', 0.29707932),\n",
       "  ('caulking', 0.2964992)])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = src_toks.index(\"ٹین\") # np.random.choice(valid_idxs)\n",
    "idx = all_sims[i].argsort()[-5:][::-1]\n",
    "sims = all_sims[i][idx]\n",
    "np.array(src_toks)[i], list(zip(np.array(tgt_tokens)[idx], sims))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
