{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import fasttext\n",
    "\n",
    "src_lang = \"ur\"\n",
    "tgt_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9313, 300), (9313, 300))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_model = fasttext.load_model(f\"../../../../datasets/cc.{src_lang}.300.bin\")\n",
    "tgt_model = fasttext.load_model(f\"../../../../datasets/cc.{tgt_lang}.300.bin\")\n",
    "\n",
    "with open(f\"temp/{src_lang}_to_{tgt_lang}.json\", \"r\") as f:\n",
    "    src_to_tgt_words = json.load(f)\n",
    "    src_toks, tgt_toks = list(src_to_tgt_words.keys()), list(src_to_tgt_words.values())\n",
    "\n",
    "src_vecs = np.array([src_model.get_word_vector(tok) for tok in src_toks])\n",
    "tgt_vecs = np.array([tgt_model.get_word_vector(tok) for tok in tgt_toks])\n",
    "\n",
    "src_vecs.shape, tgt_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30462a35ca74a64935addff856ee585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2a550714f645c4becac75e3afbe391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((200000,), (200000, 300), (200000,), (200000, 300))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tgt_tokens = np.array(tgt_model.get_words())[:200_000]\n",
    "all_tgt_vectors = np.array([tgt_model.get_word_vector(tok) for tok in tqdm(all_tgt_tokens)])\n",
    "all_src_tokens = np.array(src_model.get_words())[:200_000]\n",
    "all_src_vectors = np.array([src_model.get_word_vector(tok) for tok in tqdm(all_src_tokens)])\n",
    "\n",
    "all_tgt_tokens.shape, all_tgt_vectors.shape, all_src_tokens.shape, all_src_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7450"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = np.arange(len(src_vecs))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "n_train = int(0.8 * len(src_vecs))\n",
    "train_idxs = idxs[:n_train]\n",
    "valid_idxs = idxs[n_train:]\n",
    "\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/babylonhealth/fastText_multilingual\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = learn_transformation(src_vecs[train_idxs], tgt_vecs[train_idxs])\n",
    "# R = learn_transformation(src_vecs, tgt_vecs)\n",
    "# torch.save(torch.from_numpy(R), \"temp/align/ur_to_en_300x300.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATE METHOD\n",
    "# cross-domain similarity local scaling (CSLS) method\n",
    "# https://arxiv.org/abs/1804.07745\n",
    "\n",
    "def avg_vectors(vecs, all_vectors, n=10):\n",
    "    vecs_normed = normalized(vecs)\n",
    "    all_vectors_normed = normalized(all_vectors)\n",
    "\n",
    "    similarities = np.dot(vecs_normed, all_vectors_normed.T)\n",
    "    top_n_sims_idx = np.argpartition(similarities, -n, axis=1)[:, -n:]\n",
    "    weights = similarities[np.arange(similarities.shape[0])[:, None], top_n_sims_idx]\n",
    "    weights = weights ** 2  # my customization\n",
    "    weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "    avg_vecs = (all_vectors[top_n_sims_idx] * weights[:, :, None]).sum(axis=1)\n",
    "\n",
    "    return avg_vecs\n",
    "\n",
    "def csls(src_vectrs, tgt_vectrs, all_src_vectors, all_tgt_vectors, n=10):\n",
    "    src_avg = avg_vectors(src_vectrs, all_src_vectors, n=n)\n",
    "    tgt_avg = avg_vectors(tgt_vectrs, all_tgt_vectors, n=n)\n",
    "\n",
    "    R = learn_transformation(\n",
    "        np.concatenate([src_vectrs, src_vectrs, src_avg, src_vectrs]),\n",
    "        np.concatenate([tgt_vectrs, tgt_vectrs, tgt_vectrs, tgt_avg]),\n",
    "    )\n",
    "    return R\n",
    "\n",
    "R = csls(src_vecs[train_idxs], tgt_vecs[train_idxs], all_src_vectors, all_tgt_vectors, n=10)\n",
    "# R = csls(src_vecs, tgt_vecs, all_src_vectors, all_tgt_vectors, n=10)\n",
    "# torch.save(torch.from_numpy(R), \"temp/align/ur_to_en_300x300_csls.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  سراغ target:  clue\n",
      "original similarity:  -0.11701719\n",
      "aligned similarity 1:  0.29586565\n"
     ]
    }
   ],
   "source": [
    "src_word = np.random.choice(np.array(src_toks)[valid_idxs], 1)[0]\n",
    "src_vec = src_vecs[src_toks.index(src_word)]\n",
    "\n",
    "tgt_word = src_to_tgt_words[src_word].lower()\n",
    "tgt_vec = tgt_vecs[tgt_toks.index(tgt_word)]\n",
    "\n",
    "print(\"source: \", src_word, \"target: \", tgt_word)\n",
    "print(\"original similarity: \", (src_vec @ tgt_vec) / (np.linalg.norm(src_vec) * np.linalg.norm(tgt_vec)))\n",
    "\n",
    "print(\"aligned similarity 1: \", ((src_vec @ R) @ tgt_vec) / (np.linalg.norm(src_vec) * np.linalg.norm(tgt_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 200000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sims = normalized(src_vecs @ R) @ normalized(all_tgt_vectors).T\n",
    "val_sims = all_sims[valid_idxs]\n",
    "val_sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('جسٹس',\n",
       " 'Justice',\n",
       " [('Judge', 0.5101216),\n",
       "  ('Justice', 0.45613417),\n",
       "  ('Court', 0.4475362),\n",
       "  ('Commissioner', 0.4444608),\n",
       "  ('Magistrate', 0.43128565)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = np.random.choice(idxs) # src_toks.index(\"صبر\")\n",
    "idx = all_sims[i].argsort()[-5:][::-1]\n",
    "sims = all_sims[i][idx]\n",
    "src_toks[i], src_to_tgt_words[src_toks[i]],  list(zip(all_tgt_tokens[idx], sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "gt = GoogleTranslator(source='ur', target='en')\n",
    "# with open(f\"drive/Othercomputers/mac19/personal_repos/slt/notebooks/text/temp/ur_words.json\", \"r\") as f:\n",
    "with open(f\"temp/ur_words.json\", \"r\") as f:\n",
    "    ur_words = json.load(f)\n",
    "\n",
    "translations = {}\n",
    "def translate(w):\n",
    "    if not translations.get(w, \"\"):\n",
    "        try:\n",
    "            trans = gt.translate(w)\n",
    "        except:\n",
    "            trans = \"\"\n",
    "        translations[w] = trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE\n",
    "# with open(\"drive/Othercomputers/mac19/personal_repos/slt/notebooks/text/translations.json\", \"w\") as f:\n",
    "with open(\"translations.json\", \"w\") as f:\n",
    "    json.dump({k:v for k,v in translations.items() if v}, f, ensure_ascii=False, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LOAD\n",
    "# with open(\"drive/Othercomputers/mac19/personal_repos/slt/notebooks/text/translations.json\", \"r\") as f:\n",
    "with open(\"translations.json\", \"r\") as f:\n",
    "    translations = json.load(f)\n",
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sign_language_translator as slt\n",
    "slt.utils.threaded_map(translate, [(w,) for w in ur_words if not translations.get(w, \"\")], time_delay=5e-3, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ur_to_en.json\", \"w\") as f:\n",
    "    json.dump(dict(sorted(\n",
    "        [\n",
    "            (k,v_) for k,v in translations.items() if \" \" not in k and not re.match(r\"^\\W+$\", k) and (v_:=v.removeprefix(\"the \").removeprefix(\"The \").removeprefix(\"A \").removeprefix(\"to \").strip()) and \" \" not in v_ and v_.isascii()]\n",
    "        , key=lambda x: (-len(fasttext.tokenize(x[1])),len(x[0]), x[0]))), f, ensure_ascii=False, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../fastText/alignment/align.py \\\n",
    "    --src_emb temp/align/cc.ur.300.vec --tgt_emb temp/align/cc.en.300.vec \\\n",
    "    --dico_train temp/align/dico_train.txt --dico_test temp/align/dico_test.txt \\\n",
    "    --maxload 201000 \\\n",
    "    --output ur_to_en_align \\\n",
    "    --lr 25 --niter 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
