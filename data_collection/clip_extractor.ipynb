{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make subclips from 4 camera recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copy this file to a folder containing just 1 recording session. the folder should contain some video files labeled with the camera angle and a timestamps.txt file of format:\n",
    "\n",
    "    WORD    START   END\n",
    "    free.MP4\t44732.4020864583\t44732.4021164352\n",
    "    help.MP4\t44732.4024060185\t44732.4024640046\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries  & define functions to concatenate frames for inline display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Video, clear_output, display\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def concatFrames(front, below, left, right,height=400, inter=cv2.INTER_AREA):\n",
    "\n",
    "    f  = cv2.resize(front,  (int(height/front.shape[0]*front.shape[1]), height), interpolation = inter)\n",
    "    l  = cv2.resize(below,  (int(height/below.shape[0]*below.shape[1]), height), interpolation = inter)\n",
    "    lf = cv2.resize(left,   (int(height/left .shape[0]*left .shape[1]), height), interpolation = inter)\n",
    "    rt = cv2.resize(right,  (int(height/right.shape[0]*right.shape[1]), height), interpolation = inter)\n",
    "\n",
    "    return cv2.hconcat([ f  , l , lf , rt  ])\n",
    "\n",
    "def get_frames(vid, t, first_frame, step_size, count=8, height=300):\n",
    "    frame_nums=range(first_frame,first_frame+step_size*count,step_size)\n",
    "    frames = [cv2.putText( vid.get_frame(t+ f/vid.fps).copy(), str(f),(200,300),cv2.FONT_HERSHEY_TRIPLEX,int(vid.size[0]/1080*12), (125,63,250),5) for f in frame_nums]\n",
    "    return cv2.resize(cv2.hconcat(frames),(int(count*height/vid.size[1]*vid.size[0]),height),interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personNumber  = '255'\n",
    "# sessionNumber = '4'\n",
    "# personName    = ''\n",
    "cameras = ['front', 'below', 'left', 'right']\n",
    "\n",
    "videosFolder = os.path.abspath(os.curdir)\n",
    "clipsFolder  = os.path.abspath(os.path.join(\n",
    "    os.curdir,'..','..','PSL_Clips','HFAD_Book1','Person'+str(personNumber)\n",
    "))\n",
    "if not os.path.exists(clipsFolder):\n",
    "    print('clipsFolder does not exit. making one...')\n",
    "    os.makedirs(clipsFolder)\n",
    "\n",
    "\"\"\"timestamps.txt format\n",
    "WORD    START   END\n",
    "ح.MP4\t44732.4020864583\t44732.4021164352\n",
    "حج.MP4\t44732.4024060185\t44732.4024640046\n",
    "\"\"\"\n",
    "timestamps = pd.read_csv(videosFolder+'/timestamps.txt', sep='\\t')\n",
    "first_timestamp = timestamps.at[0,'START']\n",
    "# print('got', len(timestamps), 'timestamps')\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PATHS\n",
    "videoPaths = [\n",
    "    os.path.join(videosFolder,filename)\n",
    "    for filename in [\"front.MOV\", \"below.mp4\", \"left.mp4\", \"right.mp4\"]\n",
    "]\n",
    "assert all([os.path.exists(vp) for vp in videoPaths])\n",
    "\n",
    "# LOAD\n",
    "videos = []\n",
    "for p in videoPaths:\n",
    "    vid = mpy.VideoFileClip( p )\n",
    "    # fix bug in moviepy: vertical videos have wrong shape\n",
    "    if vid.rotation in (90, 270):\n",
    "        vid = vid.resize(vid.size[::-1])\n",
    "        vid.rotation = 0\n",
    "    videos.append(vid)\n",
    "\n",
    "# TRANSFORM VIDEO\n",
    "# fix bug in moviepy: rotate with +0.0001 and then crop\n",
    "videos[1] = videos[1].add_mask().rotate( 90.0001) .crop(x1=1,y1=1,x2=videos[1].size[1]+1,y2=videos[1].size[0]+1)\n",
    "# videos[2].fx(mpy.vfx.speedx, 1)\n",
    "# videos[3].set_fps(30)\n",
    "# videos[1]=videos[1].fx(mpy.vfx.mirror_x)\n",
    "# img_clip = mpy.ImageClip('../../../img6.png').set_pos(('left', 'top'))\n",
    "# clip = mpy.CompositeVideoClip([clip, img_clip.set_duration(clip.duration)])\n",
    "\n",
    "# SANITY CHECK\n",
    "display(Image.fromarray(cv2.hconcat([   cv2.resize(videos[0].get_frame(videos[0].duration/2),(225,400),interpolation=cv2.INTER_AREA),\n",
    "                                        cv2.resize(videos[1].get_frame(videos[0].duration/2),(225,400),interpolation=cv2.INTER_AREA)])))\n",
    "print(videoPaths)\n",
    "print([vid.duration for vid in videos])\n",
    "print([vid.fps      for vid in videos])\n",
    "print([vid.size     for vid in videos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### determine the syncronising point in every video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time at 4 frames before dark frame (when lights were turned off, measured with Avidemux) + adjustment\n",
    "sync_time = [    07.873 +  0   /videos[0].fps  , # front\n",
    "                 45.981 +  0   /videos[1].fps  , # below\n",
    "                 04.477 +  0   /videos[2].fps  , # left\n",
    "                 20.885 +  0   /videos[3].fps  ] # right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNC CHECK\n",
    "front_frames = get_frames(videos[  0  ], sync_time[ 0 ], first_frame = 0, step_size = 1)\n",
    "below_frames = get_frames(videos[  1  ], sync_time[ 1 ], first_frame = 0, step_size = 1)\n",
    "left_frames  = get_frames(videos[  2  ], sync_time[ 2 ], first_frame = 0, step_size = 1)\n",
    "right_frames = get_frames(videos[  3  ], sync_time[ 3 ], first_frame = 0, step_size = 1)\n",
    "\n",
    "display(Image.fromarray(np.concatenate([front_frames, below_frames, left_frames, right_frames],axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Start & End Time of any sign & test subclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = start_of_first_sign - dark_frame + adjustment(see below)\n",
    "start_TimeDelta = 43.471 - 20.885 +-0.1926153846153846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK\n",
    "signNum = 10\n",
    "\n",
    "startframes = [videos[i].get_frame(t+start_TimeDelta + 24*60*60*(timestamps.at[signNum,'START']-first_timestamp)) for i,t in enumerate(sync_time) ]\n",
    "endframes   = [videos[i].get_frame(t+start_TimeDelta + 24*60*60*(timestamps.at[signNum,'END'  ]-first_timestamp)) for i,t in enumerate(sync_time) ]\n",
    "\n",
    "print(timestamps.at[signNum,'WORD'])\n",
    "display(Image.fromarray( cv2.vconcat([concatFrames(*startframes, height=400),concatFrames(*endframes, height=400)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for clipNum in tqdm(range(0, len(timestamps), round(2*len(timestamps)**0.5))): # [5,25,45,65,85]: #\n",
    "        cameraNum = 0 # 0,1,2,3 #front, below, left, right\n",
    "\n",
    "        test_clip = videos[cameraNum].subclip(  sync_time[cameraNum]+start_TimeDelta + 24*60*60*(timestamps['START'].values[clipNum]-first_timestamp) ,\n",
    "                                                sync_time[cameraNum]+start_TimeDelta + 24*60*60*(timestamps['END'  ].values[clipNum]-first_timestamp) )\n",
    "        print(clipNum, timestamps.at[clipNum,'WORD'], f'{test_clip.duration:.2f} sec')\n",
    "\n",
    "        test_clip.write_videofile(videosFolder+\"/test_clip\"+str(clipNum)+timestamps.at[clipNum,'WORD']+\".mp4\", audio=False, threads = 8, verbose=False,logger=None) #test_clip.ipython_display()\n",
    "        test_clip.close()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startTime_delta ADJUSTMENT (delay at start - delay at end)\n",
    "np.mean([-0.3, -0.434, -0.234, -0.6, -0.6, -0.201, -0.7, -0.402, -0.334, -0.367, -0.367, -0.268, -0.201])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of a word\n",
    "timestamps[timestamps[\"WORD\"] == \"کیسے.MP4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check word at index\n",
    "timestamps.iloc[[12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signNumber_tqdm = tqdm(range(len(timestamps)))\n",
    "for signNum in signNumber_tqdm:\n",
    "    for cameraNum in [0,1,2,3]:\n",
    "        filename = f\"{timestamps['WORD'].values[signNum].replace('.MP4','')}_person{personNumber}_{cameras[cameraNum]}.mp4\"\n",
    "        signNumber_tqdm.set_description(filename)\n",
    "\n",
    "        clip = videos[cameraNum].subclip(\n",
    "            sync_time[cameraNum]+start_TimeDelta+ 24*60*60*(timestamps.at[signNum,'START']-first_timestamp) ,\n",
    "            sync_time[cameraNum]+start_TimeDelta+ 24*60*60*(timestamps.at[signNum,'END'  ]-first_timestamp)\n",
    "        )\n",
    "\n",
    "        filepath = os.path.join(clipsFolder, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            raise FileExistsError(filename+' already exists!')\n",
    "        clip.write_videofile(filepath, audio=False, threads=10, verbose=False,logger=None)\n",
    "        clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vid.close() for vid in videos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verify start and end frames of clips & recut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(586, 227, set())"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '/Volumes/GoogleDrive/Other computers/My Laptop/Pakistani Sign Language Translation with 3D Pose Estimation and Attention Models/Datasets/Video_Data/PSL_Clips/HFAD-Book1/person151'\n",
    "\n",
    "labels151 = list({f.split('_')[0] for f in os.listdir(base_dir) if '.mp4' in f})\n",
    "labels951 = list({f.split('_')[0] for f in os.listdir(base_dir+'/person951') if '.mp4' in f})\n",
    "len(labels151), len(labels951), set(labels151) & set(labels951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "آندھی_person151_right.mp4: 100%|██████████| 291/291 [00:06<00:00, 42.57it/s]    \n"
     ]
    }
   ],
   "source": [
    "# cam1_start, cam2_start, cam3_start, cam4_start\n",
    "# cam1_mid,   cam2_mid,   cam3_mid,   cam4_mid\n",
    "# cam1_end,   cam2_end,   cam3_end,   cam4_end\n",
    "\n",
    "for l in (bar:=tqdm(labels951[:])):\n",
    "    try:\n",
    "        first = []\n",
    "        last  = []\n",
    "        mid = []\n",
    "        if os.path.exists(f'../../../951/{l}.jpg'):\n",
    "            continue\n",
    "\n",
    "        for cam in ['front', 'below', 'left', 'right']:\n",
    "            filename = f'{l}_person151_{cam}.mp4'\n",
    "            bar.set_description(filename)\n",
    "\n",
    "            # read frames\n",
    "            cap = cv2.VideoCapture(os.path.join(base_dir, \"person951\", filename))\n",
    "            _, frame = cap.read()\n",
    "            first.append(frame)\n",
    "            last_frame_num = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(last_frame_num/2))\n",
    "            _, frame = cap.read()\n",
    "            mid.append(frame)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, last_frame_num-3)\n",
    "            _, frame = cap.read()\n",
    "            last.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            if cam != 'below':\n",
    "                # resize 0.333x\n",
    "                first[-1] = first[-1][::3,::3]\n",
    "                mid[-1] = mid[-1][::3,::3]\n",
    "                last[-1] = last[-1][::3,::3]\n",
    "\n",
    "        final = np.concatenate([\n",
    "            np.concatenate(first, axis=1)[64:512],\n",
    "            np.concatenate(mid, axis=1)[64:512],\n",
    "            np.concatenate(last, axis=1)[64:512],\n",
    "        ], axis=0)\n",
    "        final = cv2.cvtColor(final, cv2.COLOR_BGR2RGB)\n",
    "        Image.fromarray(final).save(f'../../../951/{l}.jpg')\n",
    "\n",
    "    except Exception as e:\n",
    "        print('error', l)\n",
    "    finally:\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "بےوقوف_person151_right.mp4: 100%|██████████| 4/4 [00:11<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "label = 'بےوقوف'\n",
    "start = 0.35 # sec after start\n",
    "end_ = 0 # sec before end\n",
    "for cam in (bar:=tqdm(['front', 'below', 'left', 'right'])):\n",
    "    filename = f'{label}_person151_{cam}.mp4'\n",
    "    bar.set_description(filename)\n",
    "    fpath = os.path.join(base_dir, \"person951\", filename)\n",
    "\n",
    "    clip = mpy.VideoFileClip(fpath)\n",
    "    subclip = clip.subclip(start, clip.duration-end_)\n",
    "    subclip.write_videofile(f'../../../temp/{filename}', audio=False, threads=10, verbose=False,logger=None)\n",
    "    clip.close()\n",
    "    subclip.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('slt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "444270d6efd99790c1307681015431abc1d1aa3d0ce46e81d69393c3fd1c3e19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
