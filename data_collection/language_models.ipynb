{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write 100s of sentences which must contain at least 1 ambiguous word, perplexity on bigram/trigram\n",
    "# TODO: train disambiguation model, add function to slt.languages.text.Urdu().\n",
    "# TODO: then train language model to write more sentences which are disambiguated already\n",
    "\n",
    "# WORD_SENSE_DISAMBIGUATION training pipeline (semi supervised)\n",
    "\n",
    "# train word2vec with SignTokenizer on entire data\n",
    "# label sentences for word-sense-disambiguation\n",
    "# initialize disambiguated words with ambiguous form's vectors\n",
    "# finetune just them (truncate to window size?)\n",
    "\n",
    "# TODO: Filter Wikipedia data by perplexity by training on others and evaluating on Wikipedia\n",
    "# TODO: make buckets and sample from them (more perp, less sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ? parallel corpus:\n",
    "    # PSL & urdu(transcribe)\n",
    "    # PSL(T5-summarizer) & urdu(translate)\n",
    "    # english(translate-verified)\n",
    "    # random(translate-google)\n",
    "\n",
    "# ? Vision:\n",
    "    # MP.load(\"path\")\n",
    "    # MP.load(\"path\", format=\"video\")\n",
    "    # MP.load(\"path\", format=\"blaze_pose\")\n",
    "    # MP.from_blaze_pose(\"path\")\n",
    "    # MP.from_video(\"path\")\n",
    "    # MP.save(\"path\")\n",
    "    # MP.to(format=\"numpy\")\n",
    "    # MP.to_torch\n",
    "    # MP.to_moviepy ?\n",
    "    # MP.to_video(engine = \"plt\" or \"gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLD = \"/Users/mudassar.iqbal/Library/CloudStorage/GoogleDrive-mdsriqb@gmail.com/My Drive/sign-language-translator/sign-language-datasets\"\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from importlib import reload\n",
    "import sign_language_translator as slt\n",
    "reload(slt)\n",
    "# slt.set_dataset_dir(SLD)\n",
    "\n",
    "from sign_language_translator.languages import vocab\n",
    "from sign_language_translator.languages.text import urdu\n",
    "from sign_language_translator.languages.sign import pakistan_sign_language\n",
    "from sign_language_translator.models.language_models import mixer, beam_sampling, ngram_language_model\n",
    "from sign_language_translator.models import ConcatenativeSynthesis\n",
    "from sign_language_translator.text import metrics\n",
    "from sign_language_translator.text import utils\n",
    "\n",
    "reload(vocab), reload(urdu), reload(pakistan_sign_language), reload(metrics)\n",
    "reload(mixer), reload(beam_sampling), reload(ngram_language_model), reload(utils)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ur = urdu.Urdu()\n",
    "psl = pakistan_sign_language.PakistanSignLanguage()\n",
    "t2s = ConcatenativeSynthesis(\"ur\", \"psl\", \"mp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model_training/ur_lm_training.txt\", 'r') as f:\n",
    "    data = f.read()\n",
    "data = [ur.tokenize(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model_training/token_to_id.json\") as f:\n",
    "    t2i = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af29810b3f934044a07d1513d90a3fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'sign_language_translator.models.language_models.mixer.MixerLM'\n",
       "\n",
       "Mixer LM: name=\"ur-supported-token-unambiguous-mixed-w1-w6-lm\", unk_tok=\"<unk>\"[6]\n",
       "├── Ngram LM: name=\"token-sw-w1\", unk_tok=\"<unk>\", window=1, params=14174 | prob=2.3%\n",
       "├── Ngram LM: name=\"token-sw-w2\", unk_tok=\"<unk>\", window=2, params=120596 | prob=6.6%\n",
       "├── Ngram LM: name=\"token-sw-w3\", unk_tok=\"<unk>\", window=3, params=186896 | prob=12.1%\n",
       "├── Ngram LM: name=\"token-sw-w4\", unk_tok=\"<unk>\", window=4, params=291644 | prob=18.6%\n",
       "├── Ngram LM: name=\"token-sw-w5\", unk_tok=\"<unk>\", window=5, params=250857 | prob=26.1%\n",
       "└── Ngram LM: name=\"token-sw-w6\", unk_tok=\"<unk>\", window=6, params=199543 | prob=34.3%"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sizes = [1,2,3,4,5,6]\n",
    "slms = [\n",
    "    ngram_language_model.NgramLanguageModel(window_size=w, unknown_token=\"<unk>\", name=f\"token-sw-w{w}\")\n",
    "    for w in model_sizes\n",
    "]\n",
    "[lm.fit(data) for lm in tqdm(slms, leave=True)]\n",
    "mix = mixer.MixerLM(slms, [m**1.5 for m in model_sizes], name=\"ur-supported-token-unambiguous-mixed-w1-w6-lm\")\n",
    "sampler = beam_sampling.BeamSampling(mix, beam_width=3, start_of_sequence_token = \"<\", max_length=69)\n",
    "mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.save(\"ur-supported-token-unambiguous-mixed-ngram-w1-w6-lm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = mixer.MixerLM.load(\"ur-supported-token-unambiguous-mixed-ngram-w1-w6-lm.pkl\")\n",
    "samplur = beam_sampling.BeamSampling(mux, beam_width=3, start_of_sequence_token = \"<\", max_length=69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<شوکت نے آگے میری ماں کو دیکھا۔۔۔>\n",
      "['pk-hfad-1_ش', 'pk-hfad-1_o(single-handed-letter)', 'pk-hfad-1_k(single-handed-letter)', 'pk-hfad-1_ت', 'pk-hfad-1_نے', 'pk-hfad-1_اگلا', 'pk-hfad-1_میری', 'pk-hfad-1_ماں', 'pk-hfad-1_کو', 'pk-hfad-1_دیکھا']\n"
     ]
    }
   ],
   "source": [
    "completion, log_prob = sampler(['<'])\n",
    "print(ur.detokenize(completion))\n",
    "print(t2s.translate(ur.detokenize(completion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<وہ ایک ایک دن سال سال کا اک اک لفظ سے نفرت؛ جواب اور ابا کو کچھ اور نہ تھی۔>\n",
      "['pk-hfad-1_وہ', 'pk-hfad-1_1', 'pk-hfad-1_1', 'pk-hfad-1_دن', 'pk-hfad-1_سال', 'pk-hfad-1_سال', 'pk-hfad-1_کا', 'pk-hfad-1_1', 'pk-hfad-1_1', 'pk-hfad-1_لفظ', 'pk-hfad-1_سے', 'pk-hfad-1_نفرت', 'pk-hfad-1_a(double-handed-letter)', 'pk-hfad-1_اور', 'pk-hfad-1_باپ', 'pk-hfad-1_کو', 'pk-hfad-1_چند', 'pk-hfad-1_اور', 'pk-hfad-1_نہیں', 'pk-hfad-1_تھی']\n"
     ]
    }
   ],
   "source": [
    "completion, log_prob = samplur(['<'])\n",
    "print(ur.detokenize(completion))\n",
    "print(t2s.translate(ur.detokenize(completion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' ', '؛', '!', '۔', '،', '؟'],\n",
       " [0.9443084455324341,\n",
       "  0.020807833537331663,\n",
       "  0.0030599755201958334,\n",
       "  0.015299877600979164,\n",
       "  0.015911872705018332,\n",
       "  0.0006119951040391665])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixx.next_all(\"< وہ اپنی جگہ \".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Write` to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word senses\n",
    "with open(SLD + '/text_corpora/temp/urdu_corpus_mini.txt', 'r')  as f:\n",
    "    corpus = f.read().splitlines()\n",
    "    tokenized_corpus = [ur.tokenize(l) for l in corpus]\n",
    "\n",
    "[\n",
    "    (line, [item for item in ur.get_word_senses(line) if item])\n",
    "    for line in tokenized_corpus\n",
    "    if set(line) & ambiguous_words\n",
    "][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate written sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25.245825875536944, 'آپ آؤ۔'),\n",
       " (25.33333333333333, 'آیا تھا؟'),\n",
       " (34.70700667011836, 'اپنا(ذاتی) نام تھا۔'),\n",
       " (25.33333333333333, 'تم آئے۔'),\n",
       " (28.985262713066014, 'میں آئی۔'),\n",
       " (26.920683752626147, 'میں آیا تھا۔'),\n",
       " (31.144349719464508, 'میں کام آؤں؟'),\n",
       " (38.300399931601696, 'ہم آئے۔')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f\"{slt.Settings.DATASET_ROOT_DIRECTORY}/text_corpora/temp/urdu_corpus_mini.txt\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "tokens = [[(tok+\"(i)\" if tok ==\"میں\" else tok+\"(came)\" if tok in [\"آئی\", \"آیا\"] else tok+\"(what)\" if tok == 'کیا' else tok+\"(we)\" if tok==\"ہم\"else tok ) for tok in ur.tokenize(ur.preprocess(line))] for line in lines]\n",
    "tags   = [ur.get_tags(tok) for tok in tokens]\n",
    "signs  = [psl(tokens[i], tags[i]) for i in range(len(tokens))]\n",
    "labels = [[d[\"signs\"][0][0] for d in sent] for sent in signs ]\n",
    "\n",
    "perp = metrics.Perplexity({tok for sent in labels for tok in sent})\n",
    "perp.update_frequencies(labels)\n",
    "[(perp.evaluate(l), ln) for l, ln in zip(labels, lines) if perp.evaluate(l) > 25]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('slt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "444270d6efd99790c1307681015431abc1d1aa3d0ce46e81d69393c3fd1c3e19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
