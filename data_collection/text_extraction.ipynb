{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "from importlib import reload\n",
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from urduhack.normalization import (\n",
    "    normalize,\n",
    "    normalize_characters,\n",
    "    normalize_combine_characters,\n",
    ")\n",
    "from urduhack.urdu_characters import URDU_ALL_CHARACTERS, URDU_ALPHABETS, URDU_DIACRITICS, URDU_DIGITS\n",
    "\n",
    "from sign_language_translator.text import preprocess, tokens, vocab\n",
    "\n",
    "reload(vocab), reload(tokens), reload(preprocess)\n",
    "\n",
    "stok = tokens.SignTokenizer(vocab=vocab.UNCONTEXTED_VOCAB[\"urdu\"], drop_spaces=False)\n",
    "extra_allowed_symbols = set(vocab.SYMBOLS)\n",
    "\n",
    "# char_counts = Counter(all_text)\n",
    "# [item for item in char_counts.most_common() if (item[0] not in (URDU_ALPHABETS | URDU_DIACRITICS | URDU_DIGITS |set(vocab.PUNCTUATION))) and (not item[0].lower().islower())]\n",
    "(\"—\" == '-'), (\"ـ\" == \"_\"), '”' in URDU_ALL_CHARACTERS\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRP = \"/Users/mudassar.iqbal/Library/CloudStorage/GoogleDrive-mdsriqb@gmail.com/My Drive/sign-language-translator/sign-language-datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"wikipedia\": {\n",
      "        \"n_texts\": 2648223,\n",
      "        \"n_words\": 77735304\n",
      "    },\n",
      "    \"glosbe\": {\n",
      "        \"n_texts\": 100938,\n",
      "        \"n_words\": 2204129\n",
      "    },\n",
      "    \"poetry\": {\n",
      "        \"n_texts\": 321710,\n",
      "        \"n_words\": 4782979\n",
      "    },\n",
      "    \"passage\": {\n",
      "        \"n_texts\": 104394,\n",
      "        \"n_words\": 15123833\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "with open(f'{DRP}/text_corpora/raw_corpora/wikipedia.json', 'r') as f:\n",
    "    dataset[\"wikipedia\"] = {\n",
    "        preprocess.urdu_wikipedia_preprocessor(\n",
    "            preprocess.urdu_text_normalization(line)\n",
    "        )\n",
    "        for line in json.load(f)\n",
    "    }\n",
    "with open(f'{DRP}/text_corpora/raw_corpora/glosbe.json', 'r') as f:\n",
    "    dataset[\"glosbe\"] = {\n",
    "        preprocess.urdu_text_normalization(line)\n",
    "        for line in json.load(f)\n",
    "    }\n",
    "with open(f'{DRP}/text_corpora/raw_corpora/rekhta_categories.json', 'r') as f:\n",
    "    rekhta_categories = json.load(f)\n",
    "with open(f'{DRP}/text_corpora/raw_corpora/rekhta.json', 'r') as f:\n",
    "    rekhta = json.load(f)\n",
    "    rekhta_preprocessor = {\n",
    "        \"poetry\": preprocess.urdu_poetry_preprocessor,\n",
    "        \"passage\": preprocess.urdu_passage_preprocessor,\n",
    "    }\n",
    "    for group, keys in rekhta_categories.items():\n",
    "        dataset[group] = {\n",
    "            rekhta_preprocessor[group](\n",
    "                preprocess.urdu_text_normalization(text)\n",
    "            )\n",
    "            for key in keys\n",
    "            for text in rekhta[key]\n",
    "        }\n",
    "    del rekhta\n",
    "\n",
    "print(json.dumps({k:{\"n_texts\":len(v), \"n_words\":sum([t.count(' ') for t in v])} for k,v in dataset.items()}, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract supported substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"wikipedia\"\n",
    "counts = Counter((\n",
    "    substr.strip()\n",
    "    for txt in dataset[data_name]\n",
    "    for substr in stok.extract_supported_substrings(\n",
    "        txt,\n",
    "        extra_allowed_symbols=extra_allowed_symbols - {\"بن\", \"آ\"} #-{\" \"}\n",
    "    )\n",
    "    if len(stok.tokenize(substr.strip())) > 1\n",
    "))\n",
    "sorted_counts = sorted(counts.most_common(), key=lambda item: (item[0].count(' '), item[1], item[0]), reverse=True)\n",
    "sum(counts.values()), len(counts), sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7478),\n",
       " (1, 99321),\n",
       " (2, 222926),\n",
       " (3, 206347),\n",
       " (4, 125337),\n",
       " (5, 65907),\n",
       " (6, 33831),\n",
       " (7, 17364),\n",
       " (8, 8774),\n",
       " (9, 4734),\n",
       " (10, 2643),\n",
       " (11, 1385),\n",
       " (12, 818),\n",
       " (13, 445),\n",
       " (14, 258),\n",
       " (15, 139),\n",
       " (16, 79),\n",
       " (17, 37),\n",
       " (18, 37),\n",
       " (19, 20),\n",
       " (20, 11),\n",
       " (21, 8),\n",
       " (22, 4),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (35, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_lengths = [i[0].count(' ') for i in sorted_counts]\n",
    "sentence_length_counts = Counter(sentence_lengths)\n",
    "sorted(sentence_length_counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracts[data_name] = deepcopy(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = ['poetry', \"passage\", \"glosbe\", \"wikipedia\"] # dataset.keys()\n",
    "extracts = OrderedDict({k:OrderedDict(extracts[k]) for k in data_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../sign_language_translator/datasets/text_corpora/supported_substrings_frequency.json', \"w\") as f:\n",
    "    json.dump(extracts, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del extracts\n",
    "del counts\n",
    "del sorted_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اُسکے', 'اسکے'),\n",
       " ('اُسکو', 'اسکو'),\n",
       " ('اِنہی', 'انہی'),\n",
       " ('اِنہوں', 'انہوں'),\n",
       " ('اِنہیں', 'انہیں')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(w, normalize(w)) for w in vocab.UNCONTEXTED_VOCAB['urdu'] if normalize(w) not in vocab.UNCONTEXTED_VOCAB['urdu'] and w not in vocab.SPELLED_WORDS['urdu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = {v for val in dataset.values() for v in val}\n",
    "x = dataset['passage'] | dataset['poetry']\n",
    "token_counts = Counter(stok.tokenize(' '.join(x)))\n",
    "sorted_token_counts = token_counts.most_common()\n",
    "del x\n",
    "\n",
    "remaining_words = [item for item in sorted_token_counts if item[0] not in vocab.UNCONTEXTED_VOCAB['urdu']]\n",
    "supported_words = [item for item in sorted_token_counts if item[0]     in vocab.UNCONTEXTED_VOCAB['urdu']]\n",
    "supported_words += sorted([(w,0) for w in vocab.UNCONTEXTED_VOCAB['urdu'] if w not in token_counts], key=lambda item:item[0])\n",
    "\n",
    "min_count = 10\n",
    "with open('temp/rekhta_remaining_words_frequency.json', 'w') as f:\n",
    "    json.dump(OrderedDict(\n",
    "        sorted(\n",
    "            [item for item in remaining_words if item[1] >= min_count and not (item[0].lower().islower() and len(item[0])>1)],\n",
    "            key= lambda i: (i[1], i[0]), reverse = True,\n",
    "        )\n",
    "    ), f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# with open('temp/supported_words_frequency.json', 'w') as f:\n",
    "#     json.dump(OrderedDict(supported_words), f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('slt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "444270d6efd99790c1307681015431abc1d1aa3d0ce46e81d69393c3fd1c3e19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
