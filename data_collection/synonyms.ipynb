{"cells":[{"cell_type":"markdown","metadata":{"id":"mJLngHT9YSsy"},"source":["# Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676910079746,"user":{"displayName":"mdsriqb","userId":"06721436304296357087"},"user_tz":-300},"id":"8sm_w5m6ItxY"},"outputs":[],"source":["# FOR GOOGLE COLAB ONLY\n","mount_dir = \"/content/drive\""]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9053,"status":"ok","timestamp":1676910088795,"user":{"displayName":"mdsriqb","userId":"06721436304296357087"},"user_tz":-300},"id":"Me7nrcDoItxg","outputId":"3d55bfb2-3c01-45fe-ddcf-79f939d50ee2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Othercomputers/My MacBook Pro/sign_language_translator/notebooks/data_collection\n"]}],"source":["# FOR GOOGLE COLAB ONLY\n","from IPython.display import clear_output\n","\n","# !pip install urduhack\n","!pip install deep_translator\n","clear_output()\n","\n","from google.colab import drive\n","\n","drive.mount(mount_dir)\n","\n","# install the package\n","import sys\n","\n","sys.path.append(f'{mount_dir}/Othercomputers/mac19/sign-language-translator')\n","\n","# change directory\n","%cd \"/content/drive/Othercomputers/mac19/personal_repos/slt/notebooks/data_collection\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZLHq3Wh8Itxg"},"outputs":[],"source":["import ast\n","import json\n","import os\n","import re\n","from glob import glob\n","\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from collections import Counter\n","\n","tqdm.pandas()\n","from deep_translator import GoogleTranslator\n","from IPython.display import clear_output\n","\n","from sign_language_translator.data_collection.synonyms import make_translations\n","\n","pd.set_option('display.max_colwidth', None)\n","gt = GoogleTranslator(source=\"auto\", target=\"de\")\n","gt_supported_langs = list(gt.get_supported_languages(as_dict=True).values())\n","\n","import random\n","\n","random.seed(0)\n","random.shuffle(gt_supported_langs)\n","clear_output()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# !pip install urduhack\n","import json\n","\n","from urduhack.normalization import (normalize, normalize_characters,\n","                                    normalize_combine_characters)\n","from urduhack.normalization.character import CORRECT_URDU_CHARACTERS\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"W6G9OgE5YOEH"},"source":["# Synonyms by translation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiRuvyLvItxg"},"outputs":[],"source":["# # MAKE TABLE\n","# df = pd.DataFrame({\"labels\": recording_labels[\"pk-hfad-1\"]})\n","# df.to_csv('translations.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqVtl2jVItxg","outputId":"38a2b693-3386-4c74-9d80-2787f4317493"},"outputs":[{"data":{"text/plain":["748"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# SELECT ROWS TO TRANSLATE\n","df = pd.read_csv('translations.csv')\n","valid_rows = ~(\n","    df['labels'].str.contains('-handed-')\n","    | ((df['labels'].str.len() == 1) & ~df['labels'].str.isdigit())\n","    | (df['labels']=='آ(حرف)')\n",")\n","sum(valid_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcogSFdKItxg"},"outputs":[],"source":["# MAKE TEXT LIST TO TRANSLATE\n","if not os.path.isdir(\"synonyms\"):\n","    os.makedirs(\"synonyms\")\n","with open(\"synonyms/words.txt\", 'w') as f:\n","    f.write('\\n'.join(df.loc[valid_rows, 'labels'].to_list()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UalAs2I6Itxh"},"outputs":[],"source":["# TRANSLATE INTO INTERMEDIATE LANGUAGES\n","intermediate_languages = [lang for lang in gt_supported_langs if not os.path.exists(f'synonyms/{lang}_from_words.txt')]\n","args_list = [('synonyms/words.txt', lang, 0.25) for lang in intermediate_languages]\n","\n","make_translations(args_list, n_processes=10)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cGOALfhAMJNj"},"outputs":[],"source":["# TRANSLATE TO TARGET LANGUAGE\n","target_languages = ['en', 'ur']\n","args_list = [(f'synonyms/{ilang}_from_words.txt', tlang, 0.25) for ilang in gt_supported_langs for tlang in target_languages if not os.path.exists(f'synonyms/{tlang}_from_{ilang}_from_words.txt') and os.path.exists(f'synonyms/{ilang}_from_words.txt')]\n","\n","make_translations(args_list, n_processes=10)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6izLwuphZhpc"},"outputs":[],"source":["# COMPILE TRANSLATIONS\n","target_languages = ['en', 'ur']\n","synonyms = {lang:[] for lang in ['words']+target_languages}\n","\n","# makes table with {words: [words], en:[counters], ur:[counters]}\n","with open('synonyms/words.txt', 'r') as f:\n","    for i, line in enumerate(f.read().splitlines()):\n","        synonyms['words'].append(line)\n","        for lang in target_languages:\n","            synonyms[lang].append(dict())\n","\n","# count synonyms\n","for lang in target_languages:\n","    synonyms_fpaths = glob(f'synonyms/{lang}_from_*_from_words.txt')\n","    for fpath in synonyms_fpaths:\n","        with open(fpath, 'r') as f:\n","            for i, line in enumerate(f.read().splitlines()):\n","                line = line.lower()\n","                if line not in synonyms[lang][i]:\n","                    synonyms[lang][i][line]=0\n","                synonyms[lang][i][line] += 1\n","\n","# sort synonyms by frequency\n","for lang in target_languages:\n","    for i in range(len(synonyms[lang])):\n","        synonyms[lang][i] = [\n","            w\n","            for w, count in sorted(\n","                synonyms[lang][i].items(),\n","                key=lambda item:item[1],\n","                reverse=True\n","            )\n","        ]\n","\n","# make table\n","synonyms_df = pd.DataFrame(synonyms).rename(\n","    columns={lang:f'{lang}_synonyms' for lang in target_languages}\n",")\n","# synonyms_df.to_csv('synonyms.csv', index=False)\n","synonyms_df.sample(10)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"_TKrB3dBoPyH"},"outputs":[],"source":["df = pd.read_csv('synonyms.csv')"]},{"cell_type":"markdown","metadata":{"id":"T3UxPRuNYWMg"},"source":["# Synonyms by embeddings"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"r_tnvCvgYZdh","outputId":"9b008306-043b-4e68-bd10-af34a89de6e8"},"outputs":[{"data":{"text/plain":["(461449, 0.9484775132246467)"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["embedding_model_path = '../../slt_ai/slt_ai/static/txt/W2V300dim5winBulk.txt'\n","\n","from gensim.models import KeyedVectors\n","ur_embd_model = KeyedVectors.load_word2vec_format(embedding_model_path)\n","\n","ur_vocab = set()\n","with open(embedding_model_path, 'r') as f:\n","    next(f)\n","    for line in f:\n","        ur_vocab.add(line.split()[0])\n","len(ur_vocab), sum((w==normalize(w) for w in ur_vocab))/len(ur_vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezA5aA5woPyJ","outputId":"bb90020e-fb62-46b0-fc1b-f5d45891590d"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 748/748 [00:20<00:00, 35.90it/s] \n"]}],"source":["df.loc[~df.words.isna(), 'ur_similar'] = df.loc[~df.words.isna(), 'words'].progress_apply(\n","    lambda x: set(\n","        [w for w, _ in ur_embd_model.most_similar(\n","            re.sub('\\(.*\\)', '', x).replace('-', ''),\n","            topn=15)]\n","        if re.sub('\\(.*\\)', '', x).replace('-', '') in ur_embd_model else []))"]},{"cell_type":"markdown","metadata":{"id":"V2-iJJXUoPyJ"},"source":["# view"]},{"cell_type":"code","execution_count":106,"metadata":{"id":"Vhv2KDdnoPyO","outputId":"46e3e8f3-28d3-4d7b-aed5-d605cf64edf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["len(word) = 5\n","سلیٹی\n","True\n","True\n","set()\n","س\n","ل\n","ی\n","ٹ\n","ی\n"]}],"source":["word = \"سلیٹی\"\n","print(f'{len(word) = }')\n","print(normalize(word))\n","print(normalize(word) == word)\n","print(set(word.split(' ')) <= ur_vocab)\n","print(set(word)-set(CORRECT_URDU_CHARACTERS))\n","print('\\n'.join(word))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0zW-e9noPyO","outputId":"8104b801-b932-45a1-c11e-1b59a42f8f20"},"outputs":[],"source":["df.sort_values('labels')[820:].head()[['labels', 'en_synonyms']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3gigXXGoPyO","outputId":"cecc1507-3c3c-4d6d-bc4b-058ac35a4ed1"},"outputs":[{"data":{"text/plain":["True"]},"metadata":{},"output_type":"display_data"}],"source":["word == 'کنجوسی'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYiLAxqDoPyP","outputId":"bd2a1304-2b86-4b1d-bb9b-ef12bd5fc6e6"},"outputs":[],"source":["ur_embd_model.most_similar('آجکل')"]},{"cell_type":"markdown","metadata":{},"source":["# ReView"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DRP = \"/Users/mudassar.iqbal/Library/CloudStorage/GoogleDrive-mdsriqb@gmail.com/My Drive/sign-language-translator/sign-language-datasets\""]},{"cell_type":"code","execution_count":293,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(videos) = 788\n","len(done_en) = 788\t100.0%\n","len(done_ur) = 788\t100.0%\n","len(english) = 1580\t2.01x\tlen(eng_unique) = 1466\t1.86x\n","len(urdu) = 1950\t2.47x\tlen(urdu_unique) = 1883\t2.39x\n","len(unnormalized) = 41\n","len(constructable) = 35\n"]}],"source":["# STATS\n","filepath = DRP+\"/sign_recordings/collection_to_label_to_language_to_words.json\"\n","collection = \"pk-hfad-1\"\n","\n","with open(filepath, 'r') as f:\n","    mapper = json.load(f)\n","\n","videos  = list(mapper[collection].keys())\n","english = [word for group in mapper[collection].values() for word in group[\"english\"]]\n","urdu    = [word for group in mapper[collection].values() for word in group[\"urdu\"]]\n","done_en = [k for k,v in mapper[collection].items() if len([w for w in v['english'] if w!=\"_______\"]) > 0]\n","done_ur = [k for k,v in mapper[collection].items() if len([w for w in v['urdu'] if w!=\"_______\"]) > 0]\n","eng_unique = {re.sub(r\"\\(.*\\)\", \"\", word) for word in english}\n","urdu_unique = {re.sub(r\"\\(.*\\)\", \"\", word) for word in urdu}\n","\n","print(f'{len(videos) = }\\n{len(done_en) = }\\t{len(done_en)/len(videos):.1%}\\n{len(done_ur) = }\\t{len(done_ur)/len(videos):.1%}\\n{len(english) = }\\t{len(english)/len(videos):.2f}x\\t{len(eng_unique) = }\\t{len(eng_unique)/len(videos):.2f}x\\n{len(urdu) = }\\t{len(urdu)/len(videos):.2f}x\\t{len(urdu_unique) = }\\t{len(urdu_unique)/len(videos):.2f}x')\n","\n","assert len(set(english)) == len(english), {k:v for k,v in Counter(english).items() if v > 1}\n","assert len(set(urdu))    == len(urdu),    {k:v for k,v in Counter(urdu).items()    if v > 1}\n","\n","unnormalized = [word for word in urdu if normalize_characters(word) != word]\n","assert len(unnormalized) == 0, unnormalized\n","unnormalized = [word for word in urdu if normalize_combine_characters(word) != word]\n","assert len(unnormalized) == 0, unnormalized\n","unnormalized = [word for word in urdu if normalize(word) != word]\n","# assert len(unnormalized) == 0, unnormalized # allow diacrits\n","print(f\"{len(unnormalized) = }\")#, unnormalized\n","\n","constructable = [group['components'] for group in mapper[collection].values() if \"components\" in group]\n","print(f\"{len(constructable) = }\")\n","components = [comp for comps in constructable for comp in comps if comp.replace(f\"{collection}/\", \"\") not in mapper[collection]]\n","assert len(components) == 0, components"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ambiguous_to_contexted = {\"keys\":{}, \"english\":{}, \"urdu\":{}}\n","for lang, words in [(\"keys\", videos), ('english', english), ('urdu', urdu),]:\n","    for word in words:\n","        if \"(\" in word:\n","            ambiguous = re.sub(r'\\(.*\\)', '', word)\n","            if ambiguous not in ambiguous_to_contexted[lang]:\n","                ambiguous_to_contexted[lang][ambiguous] = []\n","            ambiguous_to_contexted[lang][ambiguous].append(word)\n","\n","# drop words without any alternates (only 1 contexted option)\n","for lang in ambiguous_to_contexted:\n","    amb_words = list(ambiguous_to_contexted[lang].keys())\n","    for amb in amb_words:\n","        if len(ambiguous_to_contexted[lang][amb]) == 1:\n","            ambiguous_to_contexted[lang].pop(amb)\n","\n","print(json.dumps(ambiguous_to_contexted, indent=4, ensure_ascii=False, sort_keys=True))"]},{"cell_type":"markdown","metadata":{},"source":["# Names"]},{"cell_type":"code","execution_count":423,"metadata":{},"outputs":[],"source":["# https://www.cle.org.pk/information/people/hudasarfraz/CR0318E.pdf\n","# https://www.urdupoint.com/names/boys-islamic-names-urdu.html\n","# https://www.urdupoint.com/names/girls-islamic-names-urdu.html"]},{"cell_type":"code","execution_count":458,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>english_name</th>\n","      <th>urdu_name</th>\n","      <th>gender</th>\n","      <th>frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>306</th>\n","      <td>rizwan</td>\n","      <td>رضوان</td>\n","      <td>M</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>231</th>\n","      <td>faheem</td>\n","      <td>فہیم</td>\n","      <td>M</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>farida</td>\n","      <td>فریدہ</td>\n","      <td>F</td>\n","      <td>272.0</td>\n","    </tr>\n","    <tr>\n","      <th>339</th>\n","      <td>taimoor</td>\n","      <td>تیمور</td>\n","      <td>M</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>113</th>\n","      <td>rabbia</td>\n","      <td>ربیعہ</td>\n","      <td>F</td>\n","      <td>117.0</td>\n","    </tr>\n","    <tr>\n","      <th>276</th>\n","      <td>mudassar</td>\n","      <td>مدثر</td>\n","      <td>M</td>\n","      <td>119.0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>iram</td>\n","      <td>ارم</td>\n","      <td>F</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>270</th>\n","      <td>majeed</td>\n","      <td>مجید</td>\n","      <td>M</td>\n","      <td>2521.0</td>\n","    </tr>\n","    <tr>\n","      <th>106</th>\n","      <td>neha</td>\n","      <td>نیہا</td>\n","      <td>F</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>157</th>\n","      <td>syeda</td>\n","      <td>سیدہ</td>\n","      <td>F</td>\n","      <td>331.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    english_name urdu_name gender  frequency\n","306       rizwan     رضوان      M        NaN\n","231       faheem      فہیم      M        NaN\n","36        farida     فریدہ      F      272.0\n","339      taimoor     تیمور      M        NaN\n","113       rabbia     ربیعہ      F      117.0\n","276     mudassar      مدثر      M      119.0\n","63          iram       ارم      F        NaN\n","270       majeed      مجید      M     2521.0\n","106         neha      نیہا      F        NaN\n","157        syeda      سیدہ      F      331.0"]},"execution_count":458,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"names.csv\")\n","df.sample(10)"]},{"cell_type":"code","execution_count":445,"metadata":{},"outputs":[],"source":["with open('names.json', 'w') as f:\n","    json.dump({\n","        \"english\": df.english_name.to_list(),\n","        \"urdu\": df.urdu_name.to_list(),\n","    }, f, indent=4, ensure_ascii=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.16 ('slt')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"vscode":{"interpreter":{"hash":"444270d6efd99790c1307681015431abc1d1aa3d0ce46e81d69393c3fd1c3e19"}}},"nbformat":4,"nbformat_minor":0}
